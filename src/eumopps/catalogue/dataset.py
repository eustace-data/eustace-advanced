"""Classes to describe data sets."""

__version__ = "$Revision: 1019 $"
__author__ = "Joel R. Mitchelson"

import os
from eumopps.timeutils import datetime_dictionary
import namepatterns
import search
from checksum import Checksum
from storage import layout_builder
from threading import Thread
from threading import Event

CHECKSUM_NUM_THREADS = 64
"""Number of threads to use when computing checksums for each data subset."""


class CatalogueDataSet(object):
    """Multiple subsets of file entries."""

    def __init__(self, name=None, path=None, toolchain=None, subsets=None, non_matching=None):
        """Initialise with given values."""

        self.name = name
        self.path = path
        self.toolchain = toolchain
        self.subsets = subsets
        self.non_matching = non_matching

    def subset(self, subsetindex):
        """Specified subset, or None if index is out of range."""

        if (subsetindex >= 0) and (subsetindex < len(self.subsets)):
            return self.subsets[subsetindex]

    def search(self):
        """Perform a search across all subsets of this data sets for all component files matching this descriptor,
           and populate subsets and non_matching lists appropriately. Also searches inside any zipped files."""

        # Find all files in descriptor path
        candidates = [namepatterns.NameCandidate(filename) for filename in search.find_all_files(self.path)]

        # Match files for each subset and append to subset item
        for subset in self.subsets:
            subset.search(self.path, candidates)

        # Record anything that wasn't matched
        self.non_matching = [candidate.get_name() for candidate in candidates if not candidate.is_matched()]

    def checksum(self):
        """Compute checksums on all matching files in all subsets."""

        for subset in self.subsets:
            subset.compute_checksum(self.path)


class CatalogueDataSubset(object):
    """
    A list of file entries generated by searching for files matching a given pattern.
    """

    def __init__(self, layout=None, matches=None, value_errors=None, archive_unused=None):
        """Construct from parameters."""

        self.layout = layout
        self.matches = matches if matches else []
        self.value_errors = value_errors if value_errors else []
        self.archive_unused = archive_unused if archive_unused else []

    def search(self, datasetpath, candidates):
        """Perform a search for all component files matching this subset descriptor,
           populate lists appropriately, and set matched flag on candidates.
           Also searches inside any zipped files."""
        self.matches = []
        self.value_errors = []
        self.archive_unused = []
        self.layout.match_files(self, datasetpath, candidates)

    @staticmethod
    def compute_checksum_thread_indices(total_matches, numthreads):
        """Indices used to split processing for multi-threaded computation of checksum."""

        if total_matches == 0:
            return [ ]
        step = total_matches / numthreads
        if step < 1:
            step = 1
        thread_indices = [(start, start + step) for start in range(0, (total_matches / step) * step, step)]
        thread_indices[-1] = (thread_indices[-1][0], total_matches)
        return thread_indices

    def compute_checksum(self, datasetpath):
        """Compute checksums on all matching files.
           Note this is multi-threaded using a maximum of CHECKSUM_NUM_THREADS."""

        thread_indices = CatalogueDataSubset.compute_checksum_thread_indices(len(self.matches), CHECKSUM_NUM_THREADS)
        threads = []
        for indexstart, indexstop in thread_indices:
            thread = ChecksumThread(datasetpath, self.matches, indexstart, indexstop)
            thread.start()
            threads.append(thread)
        for thread in threads:
            thread.iscomplete.wait()

    def append_results(self, itemprefix, resultstext):
        """Parse the specified list of SearchResultsText and append."""

        for extracted in resultstext:
            try:
                desc = CatalogueFileEntry().from_extracted_text_fields(itemprefix, extracted)
                self.matches.append(desc)
            except ValueError:
                self.value_errors.append(extracted.name)

    def append_file_results(self, resultstext):
        """Append CatalogueFileEntry instance for each filename string in resultstext."""

        self.append_results('', resultstext)

    def append_archive_contents(self, archivename, archivecontents):
        """Append all ignored files, prefixed with archive name and colon."""

        itemprefix = archivename + ':'
        self.append_results(itemprefix, archivecontents)

    def append_archive_unused(self, archivename, unused):
        """Append list of unused archive elements prefixed by archive name."""

        itemprefix = archivename + ':'
        self.archive_unused.append([itemprefix + name for name in unused])


class CatalogueFileEntry(object):
    """
    A structured set of fields taken from the pathname of one file according to a pathname pattern.

    Lists of these are more easily searchable and sortable than the corresponding ExtractedTextFields
    objects from which they are generated, but construction may fail if the filename pattern does
    not contain recognised fields.
    """

    def __init__(self, name=None, time=None, tags=None, checksum=None, size=None):
        """Create from strings and lists of strings, as required for reading JSON input."""
        self.name = name
        self.time = time
        self.tags = tags
        self.checksum = checksum
        self.size = size
        # self.tags_hash = namepatterns.hash_string_list(self.tags)

    def from_extracted_text_fields(self, itemprefix, extracted):
        """Create from information taken from filename."""

        self.name = itemprefix + extracted.name
        self.time = datetime_dictionary.parse(extracted.fields)
        self.tags = []
        self.checksum = None
        self.size = None
        for key in namepatterns.TAG_FIELDS:
            if key in extracted.fields:
                self.tags.append(extracted.fields[key])
            else:
                # gaps are not allowed
                # break here to make sure we only process
                # tag fields which occur sequentially
                break
        # self.tags_hash = namepatterns.hash_string_list(self.tags)
        return self

    def compute_checksum(self, datasetpath):
        """Compute checksum for file using specified data set path."""

        try:
            pathname = os.path.join(datasetpath, self.name)
            result = Checksum(pathname)
            self.checksum = result.checksum
            self.size = result.size

        except Exception:  # pylint: disable=broad-except
            # Just allow silent failure here - error is evident by checksum
            # remaining None
            pass

class ChecksumThread(Thread):
    """Helper class to run checksum computation on a subset of match classes."""

    def __init__(self, datasetpath, matches, indexstart, indexstop):
        """Construct and set range of computation."""
        Thread.__init__(self)
        self.iscomplete = Event()
        self.datasetpath = datasetpath
        self.matches = matches
        self.indexstart = indexstart
        self.indexstop = indexstop
        self.result = []

    def run(self):
        """Override of Thread.run to run compute_checksum over the required subset."""
        for index in range(self.indexstart, self.indexstop):
            self.matches[index].compute_checksum(self.datasetpath)
        self.iscomplete.set()
